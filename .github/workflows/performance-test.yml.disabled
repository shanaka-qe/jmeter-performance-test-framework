# GitHub Actions Workflow for JMeter Performance Testing
# This workflow runs performance tests on pull requests and main branch
# It validates test plans, runs smoke tests, and generates reports

name: Performance Test Pipeline

on:
  # Trigger on pull requests to main branch
  pull_request:
    branches:
      - main
      - master
  # Trigger on push to main branch
  push:
    branches:
      - main
      - master
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        default: 'smoke'
        type: choice
        options:
          - smoke
          - load
      environment:
        description: 'Target environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - stage

# Environment variables used across all jobs
env:
  JMETER_VERSION: 5.6.3
  RESULTS_DIR: results
  REPORTS_DIR: reports

jobs:
  # Job 1: Validate JMeter test plans
  validate:
    name: Validate Test Plans
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Download and install JMeter
        run: |
          wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz
          tar -xzf apache-jmeter-${JMETER_VERSION}.tgz
          echo "JMETER_HOME=$(pwd)/apache-jmeter-${JMETER_VERSION}" >> $GITHUB_ENV
          echo "$(pwd)/apache-jmeter-${JMETER_VERSION}/bin" >> $GITHUB_PATH

      - name: Validate JMeter installation
        run: |
          jmeter --version

      - name: Validate test plans (dry run)
        run: |
          echo "Validating smoke test plan..."
          jmeter -n -t scripts/test-plans/smoke-test.jmx -q env/smoke.properties -l /dev/null -j /dev/stdout | grep -i "error" && exit 1 || true
          
          echo "Validating load test plan..."
          jmeter -n -t scripts/test-plans/load-test.jmx -q env/dev.properties -l /dev/null -j /dev/stdout | grep -i "error" && exit 1 || true
          
          echo "âœ“ All test plans are valid"

  # Job 2: Run smoke test
  smoke-test:
    name: Run Smoke Test
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Download and install JMeter
        run: |
          wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz
          tar -xzf apache-jmeter-${JMETER_VERSION}.tgz
          echo "JMETER_HOME=$(pwd)/apache-jmeter-${JMETER_VERSION}" >> $GITHUB_ENV
          echo "$(pwd)/apache-jmeter-${JMETER_VERSION}/bin" >> $GITHUB_PATH

      - name: Create results and reports directories
        run: |
          mkdir -p ${RESULTS_DIR}
          mkdir -p ${REPORTS_DIR}

      - name: Run smoke test
        env:
          TARGET_HOST: ${{ secrets.DEV_API_HOST || 'dev-api.example.com' }}
          API_KEY: ${{ secrets.API_KEY }}
          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
        run: |
          echo "Starting smoke test..."
          jmeter -n \
            -t scripts/test-plans/smoke-test.jmx \
            -q env/smoke.properties \
            -Jtarget.host=${TARGET_HOST} \
            -Japi.key=${API_KEY} \
            -Jauth.token=${AUTH_TOKEN} \
            -l ${RESULTS_DIR}/smoke-test.jtl \
            -j ${RESULTS_DIR}/smoke-test.log \
            -e \
            -o ${REPORTS_DIR}/smoke-test

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-results
          path: |
            ${{ env.RESULTS_DIR }}/smoke-test.jtl
            ${{ env.RESULTS_DIR }}/smoke-test.log
          retention-days: 30

      - name: Upload HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: smoke-test-report
          path: ${{ env.REPORTS_DIR }}/smoke-test/
          retention-days: 30

      - name: Check for test failures
        run: |
          # Extract error count from JTL file
          ERROR_COUNT=$(grep -c "false" ${RESULTS_DIR}/smoke-test.jtl || echo "0")
          TOTAL_COUNT=$(wc -l < ${RESULTS_DIR}/smoke-test.jtl)
          TOTAL_COUNT=$((TOTAL_COUNT - 1)) # Subtract header line
          
          if [ "$TOTAL_COUNT" -eq 0 ]; then
            echo "âŒ No samples were executed"
            exit 1
          fi
          
          ERROR_RATE=$(awk "BEGIN {printf \"%.2f\", ($ERROR_COUNT / $TOTAL_COUNT) * 100}")
          
          echo "ðŸ“Š Test Summary:"
          echo "Total Samples: $TOTAL_COUNT"
          echo "Errors: $ERROR_COUNT"
          echo "Error Rate: ${ERROR_RATE}%"
          
          # Fail if error rate exceeds threshold
          if (( $(echo "$ERROR_RATE > 5.0" | bc -l) )); then
            echo "âŒ Error rate ${ERROR_RATE}% exceeds threshold of 5%"
            exit 1
          fi
          
          echo "âœ“ Smoke test passed"

  # Job 3: Run quality gates validation
  quality-gates:
    name: Validate Quality Gates
    runs-on: ubuntu-latest
    needs: smoke-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          name: smoke-test-results
          path: ${{ env.RESULTS_DIR }}

      - name: Set up Python for analysis
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --no-cache-dir pandas numpy

      - name: Run quality gates validation
        run: |
          python utils/quality-gates.py \
            --jtl-file ${RESULTS_DIR}/smoke-test.jtl \
            --error-rate-threshold 2.0 \
            --p95-threshold 800 \
            --throughput-threshold 1

      - name: Generate summary
        if: always()
        run: |
          echo "## Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Smoke Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Test executed successfully" >> $GITHUB_STEP_SUMMARY
          echo "- Quality gates validated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š [View detailed HTML report in artifacts]" >> $GITHUB_STEP_SUMMARY

  # Job 4: Load test (manual or nightly)
  load-test:
    name: Run Load Test
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'load'
    needs: validate
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Download and install JMeter
        run: |
          wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz
          tar -xzf apache-jmeter-${JMETER_VERSION}.tgz
          echo "JMETER_HOME=$(pwd)/apache-jmeter-${JMETER_VERSION}" >> $GITHUB_ENV
          echo "$(pwd)/apache-jmeter-${JMETER_VERSION}/bin" >> $GITHUB_PATH

      - name: Create results and reports directories
        run: |
          mkdir -p ${RESULTS_DIR}
          mkdir -p ${REPORTS_DIR}

      - name: Run load test
        env:
          TARGET_HOST: ${{ secrets[format('{0}_API_HOST', github.event.inputs.environment)] }}
          API_KEY: ${{ secrets.API_KEY }}
          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
        run: |
          echo "Starting load test on ${{ github.event.inputs.environment }} environment..."
          jmeter -n \
            -t scripts/test-plans/load-test.jmx \
            -q env/${{ github.event.inputs.environment }}.properties \
            -Jtarget.host=${TARGET_HOST} \
            -Japi.key=${API_KEY} \
            -Jauth.token=${AUTH_TOKEN} \
            -l ${RESULTS_DIR}/load-test.jtl \
            -j ${RESULTS_DIR}/load-test.log \
            -e \
            -o ${REPORTS_DIR}/load-test

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            ${{ env.RESULTS_DIR }}/load-test.jtl
            ${{ env.RESULTS_DIR }}/load-test.log
          retention-days: 90

      - name: Upload HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: load-test-report
          path: ${{ env.REPORTS_DIR }}/load-test/
          retention-days: 90

# Workflow permissions
permissions:
  contents: read
  pull-requests: write
  checks: write

